---
title: "Regresión Lineal Simple"
author: "Equipo X"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
  - \usepackage{fancyhdr}
output:
   pdf_document:
    toc: True
    highlight: 'kate'
    number_sections: TRUE
editor_options: 
mainfont: Bookman Old Style
---
\thispagestyle{empty}
\pagebreak
\newpage
\pagenumbering{arabic} 
\fancyhead[L]{\thepage}
\fancyfoot[C]{Equipo X}
\pagestyle{fancy}
\addtolength{\headheight}{1.0cm}
\pagestyle{fancyplain}
\rhead{\includegraphics[height=1cm]{`r here::here('ITAM.png')`}}

```{r setup, include=FALSE}

#Descargar el packete here en caso de que el archivo no knitee

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(verbose = FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
options(tinytex.verbose = TRUE)

library(tidyverse)
library(MASS)
library(GGally)
library(fBasics)
library(knitr)
library(broom)

raw_df <- read.csv("scrap price.csv")

set.seed(3234)

```

# Introducción 

La globalización causó una revolución en el mercado automotriz. La apertura al comercio y la inversión internacional creó mercados más competitivos. Los consumidores ya no tenían que resignarse a consumir únicamente manufacturas nacionales, sino que podían acceder a una amplia gama de proveedores para un mismo tipo de producto o servicio. Un caso emblemático es el sector automotriz que, a pesar de ser una industria altamente protegida, no pudo evitar la penetración de competidores extranjeros. Hoy en día los diferentes fabricantes de automóviles compiten para colocar su producto y extraer mejores márgenes. Bajo este entorno altamente competitivo es necesario que los tomadores de decisiones hagan uso de herramientas precisas que ayuden a encontrar áreas de oportunidad y ventajas comparativas. Las herramientas econométricas se presentan como una manera de asistir a los tomadores de decisiones para hacer predicciones precisas y afrontar los desafíos de la industria.

Los modelos econométricos que predominan en la literatura se centran principalmente en evaluar factores externos al diseño automotriz como determinantes del precio. No obstante, estos estudios resultan anacrónicos ante el entorno competitivo actual. La entrada de más competidores y por ende la reducción de la cuota de mercado hace que controlar los factores externos a la producción sea cada vez más complejo, inasequible y costoso. Al mismo tiempo, tal competitividad crea fuertes incentivos por reducir los costos de producción de manera que el precio final al consumidor sea el menor posible, sin sacrificar márgenes atractivos.

La innovación y el desarrollo tecnológico ayudan con esta tarea, pero se debe ser eficiente en el uso de recursos. En este trabajo se propone el uso de un modelo de regresión linear simple (MRLS) en contraposcición con un módelo de regresión lineal multiple (MRLM), para identificar el componente automotriz que tenga un mayor efecto sobre en el precio. De esta forma, la alta gerencia tendrá acceso a herramientas con base en la evidencia empírica que ayude a determinar la asignación eficiente de recursos para la investigación y desarrollo (I+D). Proponemos que un uso eficiente de I+D ayudará a abaratar costos y reducir el precio de venta, sin sacrificar la competitividad en el mercado.

## Problema de interés: Análisis del precio de los vehículos 

### Breve explicación de la base de datos "Scrap price" 

## ¿Por qué usar una regresión lineal? 



\pagebreak
\newpage

# Marco teórico 

## Conceptos básicos

El MRLS es una técnica para modelar la relación lineal entre dos variables. De manera general, el MRLS se define en la siguiente ecuación: 
y=BO + B1X1 + U
 Donde y es la variable dependiente y x la variable independiente, mientras que la variable u, denominada como la perturbación estocástica, son variables aleatorias no observables que representa a todos los factores distintos de x que afectan a y. En tanto BO y B1, representan respectivamente el coeficiente del intercepto y el coeficiente de la pendiente. El coeficiente de la pendiente es el interés principal del análisis econométrico, pues mide el efecto de la variable independiente sobre la variable independiente mantenido todos los demás factores constantes. En este caso en particular, omitiremos el análisis del coeficiente del intercepto.
 
En el caso en particular que se explora en este reporte, la variable independiente y será el precio del automóvil, y la dependiente x será determinada durante el proceso de selección de variable.

## Supuestos del modelo 

El MRLS depende de seis supuestos elementales, que deberemos verificar para determinar La validez del modelo; En primer lugar, asumimos que la relación entre Xi y Yi es lineal en parámetros; en segundo lugar, se asume que no existe una dependencia lineal entre los errores y la variable dependiente o, dicho de otra manera, Cov(Ui, Xi) = 0; en tercer lugar, se supone que el valor promedio de u en la población es igual a cero, por lo que E(U) = 0, debido a esto, dejamos BO en el modelo; en cuarto lugar, se da por supuesto la homocedasticidad, por lo que Var[Ui] = (SIGMA)^2 ;en quinto lugar, el número de las observaciones debe ser mayor que el de los parámetros; y por último, los valores de xi no debieron ser todos iguales ni existen observaciones atípicas.

## Método de selección de variables y limitaciones del modelo

Dado que la base de datos en la que se basó este reporte tiene observaciones y variable relativamente limitadas (se analizaron 205 modelos de automovil y se capturaron 23 variables diferentes) se utilizó un método de selección de variables basado en un análisis gráfico de los datos. 

El MRLS es una herramienta que ayuda a orientar a los tomadores de decisiones, más no es capaz de detallar relaciones complejas. Principalmente, se encuentra limitado por explicar la relación que existe entre una sola característica del auto y el precio, cuando los determinantes de este último son multivariados. Al no poder incorporar más información el modelo propuesto podría caer en la redundancia al crear otro tipo de ineficiencias derivadas del análisis de una relación simple. Esto dado que el modelo podría informar decisiones de inversión de recursos sin considerar relaciones más complejas. 


\pagebreak
\newpage

# Análisis exploratorio de datos

## Análisis de la base de datos

Para constituir la base de datos se registraron 23 variables para 205 modelos de automóvil, así como sus precios, correspondientes a 22 fabricantes de autos. Todas las variables corresponden a características del auto, como el número de puertas, las medidas del modelo, etc.  

Como primer filtro, se descartaron las variables no numéricas. Después, se dividía a las 13 variables restantes en tres lotes para los que se generó una matriz gráfica de la correlación de cada variable con el precio del modelo automovilístico y resto de las variables del lote. De ahí se prosiguió a un análisis gráfico en el que se determinó las variables con la mayor correlación lineal al precio de la unidad. 

## Selección de la variable explicativa 

Dado el análisis gráfico se determinó que dos variables demostraban tener la mayor relación lineal: caballos de fuerza y tamaño del motor. Dado que los caballos de fuerza representan la energía que produce el motor, que a su vez está en función del tamaño del motor, se determinó que la variable dependiente deberá ser el tamaño del motor. 


```{r Ggpairs}

numeric_df <- raw_df %>% 
  dplyr::select(wheelbase, carlength, carwidth, carheight, curbweight, enginesize, boreratio, stroke, compressionratio,
                horsepower, peakrpm, citympg, highwaympg,carbody, price)

first_set <- numeric_df %>% 
  ggpairs(columns = c(1:4, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) + 
  ggtitle("First set") + 
  theme(axis.text = element_text(size = 10))

second_set <- numeric_df %>% 
  ggpairs(columns = c(5:9, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("Second set") + 
  theme(axis.text = element_text(size = 10))

third_set <- numeric_df %>% 
  ggpairs(columns = c(10:13, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("Third set") + 
  theme(axis.text = element_text(size = 10))

print(first_set)
print(second_set)
print(third_set)



```

Cómo se puede observar las variables con las correlaciones más altas son: 

Horsepower, curbweight y enginesize con 0.808, 0.835 y 0.874 respectivamente. Aunque las tres tienen correlaciones muy altas, teóricamente hace más sentido escoger es "enginesize", ya que tenemos la evidencia que el tamaño del motor influye en el precio de un auto, además de ser la variable con la correlación más alta respecto al precio. En este sentido, podemos eliminar "horse power" porque tiene una multicolienalidad imperfecta de 0.810 con la variable que elegimos.

```{r Horsepower^Enginesize}

hp_eng.size <- numeric_df %>% 
  ggpairs(columns = c(6,10), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("HP_Eng") + 
  theme(axis.text = element_text(size = 10))

print(hp_eng.size)
```
\pagebreak
\newpage

# Modelo de regresión lineal simple

## Parámetros del modelo 

En un modelo de regresión, los parámetros son los valores que se ajustan al conjunto de datos para crear la mejor línea o curva que sea la mejor representación posible de la relación entre la variable independiente y la variable dependiente.

En una regresión lineal simple, los parámetros son la pendiente y la intersección en el eje. La pendiente representa el cambio en la variable dependiente por cada cambio unitario en la variable independiente, mientras que la intersección en el eje y representa el valor de la variable dependiente cuando la variable independiente es igual a cero.

El objetivo de un modelo de regresión lineal es encontrar los valores óptimos de los parámetros que minimicen la diferencia entre las predicciones del modelo y los valores reales de la variable dependiente en el conjunto de datos.

A continuación, presentamos los siguientes valores obtenidos, para describir lo explicado anteriormente:



```{r MRLS, comment = ""}

train.base <- raw_df %>% 
  sample_frac(.70)

val.base <- raw_df %>% 
  setdiff(train.base)


modelo_1 <- lm(price~enginesize, data = train.base)

summary(modelo_1)

```
Los valores de los parámetros son B0= -8156.662 y B1= 167.62, con unos errores estándar de 1022.821 y 7.497 respectivamente.

Podemos observar que para B0 y B1 el P value < |t value | por lo tanto B0 y B1 son significativas con un nivel de confianza de 1


A continuación, la Tabla ANOVA: 

```{r ANOVA, comment = ""}

anova(modelo_1)

```
\pagebreak
\newpage

## Análisis de residuales

### Comprobación de la linealidad de la Fn de regresión

Comprobamos con la R^2, en este caso los errores se acercan un 77% a nuestra recta de regresión
lo que nos dice que sí hay linealidad en ella. 

Al calcular la R^2, como anteriormente se mencionó, obtenemos que es .7787968 ; esto quiere decir que la variable X explica en un 77.88% a la variable dependiente Y.

### Heterocedasticidad 

Comprobamos heterocedisticidad (que la varianza de los errores sea constante), lo comprobamos con un gráfico, donde comparamos los residuales con las Y observadas (ŷ), para esto tenemos que hacer un DF con ambos vectores obtenidos de nuestro modelo

```{r heterocedasticidad}
prueba_heter <- as.data.frame(cbind(modelo_1$fitted.values, modelo_1$residuals))

# cambiamos el nombre de nuestras columnas para una mayor facilidad de lectura 

colnames(prueba_heter) <- c("Y_observada", "Residuales")

# Graficamos para observar si hay un patrón o no respecto ambas variables

grafico_heter <- prueba_heter %>% 
  ggplot() + 
  geom_point(aes(Y_observada, Residuales)) + 
  theme_classic()

grafico_heter
  

```

Dado el anterior gráfico, podemos observar que no existe un patrón notório en el gráfico, ya sea como la silueta de una recta, con esto podemos asumir que hay heterocedisticidad, por lo tanto la varianza de nuestros errores es constante.

### Independencia en los errores 
Los datos usados en este módelo no corresponden a una serie de tiempo, por lo tanto no aplica, ya que los datos no llevan un orden y pueden cambiar de posición 
\pagebreak
\newpage

### Presencia de errores atípicos 

Este resultado lo obtenemos de la siguiente forma: Obtenemos la suma de cuadrados de los errores (MSE), de la tabla ANOVA, después calculamos la raiz cuadrada del MSE.

Ya con (MSE)^(1/2), podemos obtener la división de los residuales entre la raíz de MSE y compararlos con nuestras x'i, estas variables las metemos en un DF y graficamos las diferencias.

```{r Independencia de errores}

## Esto se hace calculando la raíz de el cuadrado medio de la suma de cuadrados de los errores (MSE)
## El cual se obtiene de la tabala ANOVA de nuestros residuales el mean 

anova_1 <- anova(modelo_1)
sqrt_mse <- sqrt(anova_1[2,3])

## Ya con MSE^(1/2), podemos sacar la división de los residuales entre la raíz de MSE y compararlos con xi
## Esas variables las metemos en un DF 

df_ea <- as.data.frame(cbind(train.base$enginesize, modelo_1$residuals/sqrt_mse))

colnames(df_ea) <- c("X", "Errores")

# Graficamos esas diferencias

grafico_ea <- df_ea %>% 
  ggplot() + 
  geom_line(aes(x = X, y = -4),col = "Red", alpha=.5) +
  geom_line(aes(x = X, y = 4), col = "Red", alpha=.5) +
  geom_point(aes(x = X, y = Errores)) + 
  ggtitle("x vs ui / raiz(mse)") +
  ylab("ui / raiz(mse)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_ea

```

Como podemos observar en el gráfico anterior no existe ningún dato atípico. 

\pagebreak
\newpage

### Verificar la normalidad en los errores 

Usaremos la función QQ plot para verificar esto; esta función se realiza con los residuales obtenidos en nuestro modelo. 

```{r QQ Plot}

ui <- as.data.frame(modelo_1$residuals)
colnames(ui) <- c("Ui")

ui %>% 
  ggplot(aes(sample = Ui)) + 
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot Residuales") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))



```

Rechazamos el supuesto de normalidad de los errores debido a las dos colas que muestra el gráfico de QQ plot. No obstante, ya que el modelo de regresión lineal simple ajustado es robusto ante el supuesto de normalidad podemos continuar usando esta variable explicativa.


\pagebreak
\newpage

## Intervalo de confianza y predicción al 95%

Sacamos el intervalo de confianza de E[Y], esto lo hacemos para ver el intervalo en donde van a estar las siguientes E[Y / X], independientemente de la muestra.

En R usamos la fn Predict.lm la alimentamos con el modelo_1 con nuestra data de entrenamiento aplicando el intervalo de confianza a el nivel requerido, en este caso .95 

```{r Confianza y predicción}

confianza <- as.data.frame(predict.lm(modelo_1, train.base, interval = "confidence", level = .95))

colnames(confianza) <- c("Y_Observada", "lwr", "upr")

# también hacemos la predicción para poder obtener ambos intervalos, los de confianza y los de predicción

prediccion <- as.data.frame(predict.lm(modelo_1, train.base, interval = "prediction", level = .95))

colnames(prediccion) <- c("Y_Observada_predecida", "lwr_pred", "upr_pred")

# Ya nos da un Df con la info de nuestra
# confianza y la predicción 

train.base.confianza.prediccion <- as.data.frame(c(train.base, confianza, prediccion))


grafico_confianza_prediccion <- train.base.confianza.prediccion %>% 
  ggplot() + 
  geom_point(aes(x = enginesize, y = price)) + 
  # Agregamos los intervalos de confianza y de predicción 
  geom_line(aes(x = enginesize, y = lwr), color = "red", linetype = "dashed") + 
  geom_line(aes(x = enginesize, y = upr), color = "red", linetype = "dashed") +
  geom_line(aes(x = enginesize, y = lwr_pred), color = "purple", linetype = "dashed") +
  geom_line(aes(x = enginesize, y = upr_pred), color = "purple", linetype = "dashed") +
  # Agregamos las rectas de E[Y / X]
  geom_line(aes(x = enginesize, y = Y_Observada), color = "blue") +
  #Agregamos la recta de Y predecida
  geom_line(aes(x = enginesize, y = Y_Observada_predecida), color = "black", linetype = "dashed") +
  ggtitle("Intervalo de predicción y confianza al 95%")+
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_confianza_prediccion

```

\pagebreak
\newpage

# Modelo de regresión lineal Múltiple 

## Selección de los regresores 

Utilizamos las variables numéricas nuevamente para poder tener un mejor control sobre las regresoras y utilizamos el método "Backward", este método es computacional, pero debido a que hay una cantidad manejable de variables lo hicimos a mano, tomando en cuenta un nivel de significancia de .05 

Iniciamos el modelo tomando en cuenta todas las variables numericas.

```{r MRLM Selección, comment = ""}


library(caTools)
library(car)


numeric_df <- raw_df %>% 
  dplyr::select(wheelbase, carlength, carwidth, carheight, curbweight, enginesize, boreratio, stroke, compressionratio,
                horsepower, peakrpm, citympg, highwaympg,carbody, price)

train.base_m <- numeric_df %>% 
  sample_frac(.70)

val.base_m <- numeric_df %>% 
  setdiff(train.base_m)

modelo_todo <- lm(price~ ., data = train.base_m)

summary(modelo_todo)

```

\pagebreak

Con los datos que obtuvimos anteriormente, podemos observar que algunos de los P-values de las regresoras son mayores a 0.05, por lo que iremos depurando las variables que no cumplen con la prueba Global de la regresión, e iremos corriendo la regresión nuevamente por cada modelo.

También utilizamos el Variance Inflation Factor (VIF) el cual nos muestra las correlaciones que existen entre los regresores, para esta prueba estamos buscando que el VIF menor a 10 en todas las variables de nuestro modelo. 


En el reporte sólo vamos a poner el modelo sin las variables con Pvalue mayor a 0.05.


```{r MRLM lm a usar, comment = "" }

modelo.8 <- lm(price~ . -citympg -curbweight -carlength -horsepower -boreratio -highwaympg -carheight -wheelbase
               -carbody, data = train.base_m)

summary(modelo.8)


```

Observando los resultados obtenidos y los requisitos expuestos, estas son las regresoras que cuentan con un VIF menor a 10:

```{r VIF, comment = "" }

print(as.data.frame(vif(modelo.8)))

```

\pagebreak
\newpage

## Análisis de residuales 

### Comprobación de la linealidad de la Fn de regresión Multiple

Ahora comprobamos nuestros resultados usando la R^2 ajustada, una vez que la calculamos, obtenemos lo siguiente: 

```{r R^2, comment = "" }

suma_m <- summary(modelo.8)
suma_m$r.squared

```

tenemos una R^2 ajustada que nos dice que sí es una regresión lineal.

### Comprobamos heterocedasticidad 

Nuevamente, en el modelo de regresión lineal multiple, debemos comporbar que la varianza de los errores sea constante, comprobamos que se cumple la heterocedasticidad. 

```{r Heterocedasticidad}

prueba_heter_m <- as.data.frame(cbind(modelo.8$fitted.values, modelo.8$residuals))

colnames(prueba_heter_m) <- c("Y_estimada", "Residuales")

grafico_heter_m <- prueba_heter_m %>% 
  ggplot() + 
  geom_point(aes(Y_estimada, Residuales)) + 
  theme_classic()

grafico_heter_m

```

Como podemos observar en el gráfico anterior, no se nota que exista un patrón evidente en los errores, lo cual verifica que sí hay heterocedasticidad. 

### Independencia en los errores 

Los datos usados en este módelo no corresponden a una serie de tiempo, por lo tanto no aplica, ya que los datos no llevan un orden y pueden cambiar de posición.  

\pagebreak
\newpage

### Presencia de errores atípicos 

```{r Errores Atípicos}

anova_m <- anova(modelo.8)
sqrt_mse_m <- sqrt(anova_m[6,3])

df_ea_m <- as.data.frame(cbind(train.base_m$carwidth, train.base_m$enginesize, train.base_m$stroke,
                                 train.base_m$compressionratio, train.base_m$peakrpm, modelo.8$residuals/sqrt_mse_m))

colnames(df_ea_m) <- c("X1", "X2", "X3", "X4", "X5", "Errores")

grafico_ea_m <- df_ea_m %>% 
  ggplot() + 
  geom_hline(yintercept = -4, colour = "red") + 
  geom_hline(yintercept = 4, colour = "red")  +
  geom_point(aes(x = X1, y = Errores, color = "Carwidth")) + 
  geom_point(aes(x = X2, y = Errores, color = "Enginesize")) +
  geom_point(aes(x = X3, y = Errores, color = "Stroke")) +
  geom_point(aes(x = X4, y = Errores, color = "Compressionratio")) +
  geom_point(aes(x = log(X5), y = Errores, color = "Peakrpm")) + # le hice una transformación lineal a los datos para poder comparar mejor
  ggtitle("x vs ui / raiz(mse)") +
  ylab("ui / raiz(mse)") +
  labs(title = "Datos atípicos") + 
  scale_color_manual(values = c("Carwidth" = "blue", "Enginesize" = "green", "Stroke" = "black", 
                                "Compressionratio" = "red", "Peakrpm" = "purple")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_ea_m


```


Observando este gráfico, podemos notar que hay errores atípicos, los cuales podemos eliminar debido a que este análisis no es una serie de tiempo, en nuestro DF es el renglón 28, por lo que lo eliminamos

Una vez que eliminamos estos datos, podemos proseguir con las siguientes verificaciones.


\pagebreak
\newpage

### Verificar la normalidad en los errores 

Para verificar este punto, usamos la función QQ plot; esta se realiza con los residuales de nuestro modelo.

```{r QQ plot M}

ui_m <- as.data.frame(modelo.8$residuals)
colnames(ui_m) <- c("Ui")

ui_m %>% 
  ggplot(aes(sample = Ui)) + 
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot Residuales") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


```


Rechazamos el supuesto de normalidad de los errores debido a las dos colas que muestra el gráfico de QQ plot. No obstante, ya que el modelo de regresión lineal Multiple ajustado es robusto ante el supuesto de normalidad podemos continuar usando estas variable.

# Comparativo entre modelos y selección de un modelo



```{r R^2 MRLS, comment= "El modelo de regresión lineal simple tiene una R^2 ajustada de "}

suma_s <- summary(modelo_1)
suma_s$adj.r.squared

```


```{r R^2 MRLM, comment = "Y el MRLM tiene una R ajustada de "}

suma_m <- summary(modelo.8)
suma_m$r.squared


dif_R <- suma_m$adj.r.squared - suma_s$adj.r.squared

```

Ya que estamos utilizando las R^2 ajustadas, las cuales si nos dejan comparar modelos, y vemos que R^2 del MRLM es mayor que la del MRLS por:

```{r, comment = ""}

dif_R

```

Lo cual nos lleva a elegir el MRLM que es el mejor ajustado para nuestro precio.

# Conclusiones 

A partir de la evidencia recolectada en los analisís que se realizaron en este trabajo, podemos concluir con seguridad que el Modelo de Regresión Lineal Multiple es mejor para este problema y explica de mejor manera el compartamiento del precio en los coches. 

Esta conclusión se puede explicar desde dos maneras; la primera, de una forma matemática, en este sentido sabemos que la R^2 (ajustada) será mejor en un modelo multiple a comparación de un modelo simple. Este razonamiento lo obtenemos a partir de la formula de la R^2 (ajustada), como sabemos a mayor número de regresoras, el valor de la R^2 (ajustada) será mayor al del modelo lineal; en segundo lugar, de forma teórica, sabemos que en muy pocos bienes, si no es que en ninguno, el precio depende únicamente de un solo factor. Por lo que, es de esperarse que un modelo acierte en mayor medida al precio de un coche si se consideran más factores. En el caso especifico de los coches, el precio se ve afectado por muchos factores. Dentro del mercado, existen muchos modelos que son iguales en casí todos los factores, con excepción de un par y la diferencia en precios se explica por esas mínimas diferencias. De igual forma, es una industra que no se diferencia por un solo factor, cada marca, cada modelo, tiene sus caracteristicas particulares y diferencias entre sus diferentes factores. 

Dado los puntos anteriores, se nota una clara mejora entre usar el modelo de regresión lineal multiple, al usar el modelo de regresion lineal simple.


# Bibliografía

>Gatto, L. (2017, 10 noviembre). Data analysis and R programming. https://lgatto.github.io/2017_11_09_Rcourse_Jena/manipulating-and-analyzing-data.html

>Hernández, F. (2020, 30 octubre). 12 Pruebas de independencia de los errores | Modelos de Regresión con R. https://fhernanb.github.io/libro_regresion/indep.html

>Chatterjee, S., & Price, B. (1980). Regression Analysis by Example. Journal of the American Statistical Association, 75(372), 1039. 

>Dalgaard, P. (2008). Introductory Statistics with R. En Springer eBooks. 



