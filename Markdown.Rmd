---
title: "Regresión Lineal Simple"
author: "Equipo X"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
  - \usepackage{fancyhdr}
output:
   pdf_document:
    toc: True
    highlight: 'kate'
    number_sections: TRUE
editor_options: 
mainfont: Bookman Old Style
---
\thispagestyle{empty}
\pagebreak
\newpage
\pagenumbering{arabic} 
\fancyhead[L]{\thepage}
\fancyfoot[C]{Equipo X}
\pagestyle{fancy}
\addtolength{\headheight}{1.0cm}
\pagestyle{fancyplain}
\rhead{\includegraphics[height=1cm]{`r here::here('ITAM.png')`}}

```{r setup, include=FALSE}

#Descargar el packete here en caso de que el archivo no knitee

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(verbose = FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
options(tinytex.verbose = TRUE)

library(tidyverse)
library(MASS)
library(GGally)
library(fBasics)
library(knitr)
library(broom)

raw_df <- read.csv("scrap price.csv")

set.seed(3234)

```

# Introducción 

## Problema de interés: Análisis del precio de los vehículos [Raúl]

### Breve explicación de la base de datos "Scrap price" [Raúl]

## ¿Por qué usar una regresión lineal? [Raúl]



\pagebreak
\newpage

# Marco teórico [AOKI]

## Conceptos básicos

## Supuestos del modelo 

## Método de selección de variables y limitaciones del modelo


\pagebreak
\newpage

# Análisis exploratorio de datos

## Análisis de la base de datos

Aquí va filtros aplicados, estadígrafos, gráficas, limpieza y selección de variable depen-
diente con justificación

## Selección de la variable explicativa 

Para la selección de la variable explicativa eliminamos las variables de caracter, y dejamos las variables numéricas. Dividimos las variables en 3 grupos para poder analizar la correlación de las variables respecto el precio:



```{r Ggpairs}

numeric_df <- raw_df %>% 
  dplyr::select(wheelbase, carlength, carwidth, carheight, curbweight, enginesize, boreratio, stroke, compressionratio,
                horsepower, peakrpm, citympg, highwaympg,carbody, price)

first_set <- numeric_df %>% 
  ggpairs(columns = c(1:4, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) + 
  ggtitle("First set") + 
  theme(axis.text = element_text(size = 10))

second_set <- numeric_df %>% 
  ggpairs(columns = c(5:9, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("Second set") + 
  theme(axis.text = element_text(size = 10))

third_set <- numeric_df %>% 
  ggpairs(columns = c(10:13, 15), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("Third set") + 
  theme(axis.text = element_text(size = 10))

print(first_set)
print(second_set)
print(third_set)



```

Cómo se puede observar las variables con las correlaciones más altas son: 

Horsepower, curbweight, enginesize con 0.808, 0.835 y 0.874 respectivamente. No obstante, la variable que más sentido hace para elegir para explicar el motor es "enginesize", ya que además de tener la correlación más alta respecto al precio, podemos eliminar "horse power" porque tiene una multicolienalidad imperfecta de 0.810 con la variable que elegimos.

```{r Horsepower^Enginesize}

hp_eng.size <- numeric_df %>% 
  ggpairs(columns = c(6,10), aes(color = carbody, alpha = .5), upper = list(continuous = wrap("cor", size = 4.5))) +
  ggtitle("HP_Eng") + 
  theme(axis.text = element_text(size = 10))

print(hp_eng.size)
```
\pagebreak
\newpage

# Modelo de regresión lineal simple

## Parámetros del modelo 

En un modelo de regresión, los parámetros son los valores que se ajustan al conjunto de datos para crear la mejor línea o curva que represente la relación entre la variable independiente y la variable dependiente.

En una regresión lineal simple, los parámetros son la pendiente y la intersección en el eje. La pendiente representa el cambio en la variable dependiente por cada cambio unitario en la variable independiente, mientras que la intersección en el eje y representa el valor de la variable dependiente cuando la variable independiente es igual a cero.

El objetivo de un modelo de regresión es encontrar los valores óptimos de los parámetros que minimicen la diferencia entre las predicciones del modelo y los valores reales de la variable dependiente en el conjunto de datos.



```{r MRLS, comment = ""}

train.base <- raw_df %>% 
  sample_frac(.70)

val.base <- raw_df %>% 
  setdiff(train.base)


modelo_1 <- lm(price~enginesize, data = train.base)

summary(modelo_1)

```
Los valores de los parámetros son B0= -8156.662 y B1= 167.62 con unos errores estándar de 1022.821 y 7.497 respectivamente.

Podemos observar que para B0 y B1 el P value < |t value | por lo tanto B0 y B1 son significativas con un nivel de confianza de 1


Tabla ANOVA 

```{r ANOVA, comment = ""}

anova(modelo_1)

```
\pagebreak
\newpage

## Análisis de residuales

### Comprobación de la linealidad de la Fn de regresión

Comprobamos con la R^2, en este caso los errores se acercan un 77% a nuestra recta de regresión
lo que nos dice que sí hay linealidad en ella, lo comprobamos sacando la R^2

nos da el .7787968 de R^2. Esto quiere decir que la variable X explica en un 77.88% a la variable dependiente Y.

### Heterocedasticidad 

Comprobamos heterocedisticidad (la varianza de los errores es constante), lo comprobamos con un gráfico comparando los residuales con las Y observadas (ŷ), para esto tenemos que hacer un DF con ambos vectores obtenidos de nuestro modelo

```{r heterocedasticidad}
prueba_heter <- as.data.frame(cbind(modelo_1$fitted.values, modelo_1$residuals))

# cambiamos el nombre de nuestras columnas para una mayor facilidad de lectura 

colnames(prueba_heter) <- c("Y_observada", "Residuales")

# Graficamos para observar si hay un patrón o no respecto ambas variables

grafico_heter <- prueba_heter %>% 
  ggplot() + 
  geom_point(aes(Y_observada, Residuales)) + 
  theme_classic()

grafico_heter
  

```

Se puede observar que no hay un patrón en sí en el gráfico, como una recta, con esto podemos asumir que hay heterocedisticidad.

### Independencia en los errores 

No es una serie de tiempo - no aplica ya que los datos no llevan un orden y pueden cambiar de pocisión 
\pagebreak
\newpage

### Presencia de errores atípicos 

Esto se hace calculando la raíz de el cuadrado medio de la suma de cuadrados de los errores (MSE), el cual se obtiene de la tabala ANOVA de nuestros residuales el mean.

Ya con MSE^(1/2), podemos sacar la división de los residuales entre la raíz de MSE y compararlos con xi esas variables las metemos en un DF y graficamos las diferencias.

```{r Independencia de errores}

## Esto se hace calculando la raíz de el cuadrado medio de la suma de cuadrados de los errores (MSE)
## El cual se obtiene de la tabala ANOVA de nuestros residuales el mean 

anova_1 <- anova(modelo_1)
sqrt_mse <- sqrt(anova_1[2,3])

## Ya con MSE^(1/2), podemos sacar la división de los residuales entre la raíz de MSE y compararlos con xi
## Esas variables las metemos en un DF 

df_ea <- as.data.frame(cbind(train.base$enginesize, modelo_1$residuals/sqrt_mse))

colnames(df_ea) <- c("X", "Errores")

# Graficamos esas diferencias

grafico_ea <- df_ea %>% 
  ggplot() + 
  geom_line(aes(x = X, y = -4),col = "Red", alpha=.5) +
  geom_line(aes(x = X, y = 4), col = "Red", alpha=.5) +
  geom_point(aes(x = X, y = Errores)) + 
  ggtitle("x vs ui / raiz(mse)") +
  ylab("ui / raiz(mse)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_ea

```

\pagebreak
\newpage

### Verificar la normalidad en los errores 

La QQ plot - esa se hace con los residuales sacamos los residuales de nuestro modelo

```{r QQ Plot}

ui <- as.data.frame(modelo_1$residuals)
colnames(ui) <- c("Ui")

ui %>% 
  ggplot(aes(sample = Ui)) + 
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot Residuales") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))



```

Rechazamos el supuesto de normalidad de los errores debido a las dos colas que muestra el gráfico de QQ plot. No obstante, ya que el modelo de regresión lineal simple ajustado es robusto ante el supuesto de normalidad podemos continuar usando esta variable explicativa.


\pagebreak
\newpage

## Intervalo de confianza y predicción al 95%

Sacamos el intervalo de confianza de E[Y], esto lo hacemos para ver el intervalo en donde van a estar las siguientes E[Y / X], independientemente de la muestra.

En R usamos la fn Predict.lm la alimentamos con el modelo_1 con nuestra data de entrenamiento aplicando el intervalo de confianza a el nivel requerido, en este caso .95 [Explicar porque (AOKI)]

```{r Confianza y predicción}

confianza <- as.data.frame(predict.lm(modelo_1, train.base, interval = "confidence", level = .95))

colnames(confianza) <- c("Y_Observada", "lwr", "upr")

# también hacemos la predicción para poder obtener ambos intervalos, los de confianza y los de predicción

prediccion <- as.data.frame(predict.lm(modelo_1, train.base, interval = "prediction", level = .95))

colnames(prediccion) <- c("Y_Observada_predecida", "lwr_pred", "upr_pred")

# Ya nos da un Df con la info de nuestra
# confianza y la predicción 

train.base.confianza.prediccion <- as.data.frame(c(train.base, confianza, prediccion))


grafico_confianza_prediccion <- train.base.confianza.prediccion %>% 
  ggplot() + 
  geom_point(aes(x = enginesize, y = price)) + 
  # Agregamos los intervalos de confianza y de predicción 
  geom_line(aes(x = enginesize, y = lwr), color = "red", linetype = "dashed") + 
  geom_line(aes(x = enginesize, y = upr), color = "red", linetype = "dashed") +
  geom_line(aes(x = enginesize, y = lwr_pred), color = "purple", linetype = "dashed") +
  geom_line(aes(x = enginesize, y = upr_pred), color = "purple", linetype = "dashed") +
  # Agregamos las rectas de E[Y / X]
  geom_line(aes(x = enginesize, y = Y_Observada), color = "blue") +
  #Agregamos la recta de Y predecida
  geom_line(aes(x = enginesize, y = Y_Observada_predecida), color = "black", linetype = "dashed") +
  ggtitle("Intervalo de predicción y confianza al 95%")+
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_confianza_prediccion

```

\pagebreak
\newpage

# Modelo de regresión lineal Múltiple 

## Selección de los regresores 

Utilizamos las variables numéricas nuevamente para poder tener un mejor control sobre las variables y utilizamos el método "Backward", este método es computacional pero debido a que hay una cantidad manejable de variables lo hicimos a mano, tomando en cuanta un nivel de significancia de .005 

Iniciamos con el modelo tomando en cuenta todas las variables del modelo

```{r MRLM Selección, comment = ""}


library(caTools)
library(car)


numeric_df <- raw_df %>% 
  dplyr::select(wheelbase, carlength, carwidth, carheight, curbweight, enginesize, boreratio, stroke, compressionratio,
                horsepower, peakrpm, citympg, highwaympg,carbody, price)

train.base_m <- numeric_df %>% 
  sample_frac(.70)

val.base_m <- numeric_df %>% 
  setdiff(train.base_m)

modelo_todo <- lm(price~ ., data = train.base_m)

summary(modelo_todo)

```

\pagebreak

Ya con esos datos podemos observar que los Pvalues son mayores a 0.005 en algunos regresores, por lo que vamos a ir quitando variables que no cumplen con la prueba Global de la regresión y corriendo la regresión nuevamente por cada modelo.

También utilizamos el Variance Inflation Factor (VIF) el cual nos da las correlaciones entre los regresores, y estamos buscando un VIF menor a 10 en todas las variables de nuestro modelo 



En el reporte sólo vamos a poner el modelo sin las variables con Pvalue mayor a 0.005

```{r MRLM lm a usar, comment = "" }

modelo.8 <- lm(price~ . -citympg -curbweight -carlength -horsepower -boreratio -highwaympg -carheight -wheelbase
               -carbody, data = train.base_m)

summary(modelo.8)


```

Como se puede observar, las variables ya tienen un VIF menos a 10 

```{r VIF, comment = "" }

print(as.data.frame(vif(modelo.8)))

```

\pagebreak
\newpage

## Análisis de residuales 

### Comprobación de la linealidad de la Fn de regresión Multiple

Lo checamos con la R^2 ajustada 

```{r R^2, comment = "" }

suma_m <- summary(modelo.8)
suma_m$r.squared

```

tenemos una R^2 ajustada que nos dice que sí es una regresión lineal.

### Comprobamos heterocedasticidad 

Esto es para poder ver si la varianza de los errores es constante. 

```{r Heterocedasticidad}

prueba_heter_m <- as.data.frame(cbind(modelo.8$fitted.values, modelo.8$residuals))

colnames(prueba_heter_m) <- c("Y_estimada", "Residuales")

grafico_heter_m <- prueba_heter_m %>% 
  ggplot() + 
  geom_point(aes(Y_estimada, Residuales)) + 
  theme_classic()

grafico_heter_m

```

Se puede observar que no hay un patrón en sí en los errores, lo cual nos dice que sí hay heterocedasticidad 

### Independencia en los errores 

No es una serie de tiempo - no aplica ya que los datos no llevan un orden y pueden cambiar de pocisión 

\pagebreak
\newpage

### Presencia de errores atípicos 

```{r Errores Atípicos}

anova_m <- anova(modelo.8)
sqrt_mse_m <- sqrt(anova_m[6,3])

df_ea_m <- as.data.frame(cbind(train.base_m$carwidth, train.base_m$enginesize, train.base_m$stroke,
                                 train.base_m$compressionratio, train.base_m$peakrpm, modelo.8$residuals/sqrt_mse_m))

colnames(df_ea_m) <- c("X1", "X2", "X3", "X4", "X5", "Errores")

grafico_ea_m <- df_ea_m %>% 
  ggplot() + 
  geom_hline(yintercept = -4, colour = "red") + 
  geom_hline(yintercept = 4, colour = "red")  +
  geom_point(aes(x = X1, y = Errores, color = "Carwidth")) + 
  geom_point(aes(x = X2, y = Errores, color = "Enginesize")) +
  geom_point(aes(x = X3, y = Errores, color = "Stroke")) +
  geom_point(aes(x = X4, y = Errores, color = "Compressionratio")) +
  geom_point(aes(x = log(X5), y = Errores, color = "Peakrpm")) + # le hice una transformación lineal a los datos para poder comparar mejor
  ggtitle("x vs ui / raiz(mse)") +
  ylab("ui / raiz(mse)") +
  labs(title = "Datos atípicos") + 
  scale_color_manual(values = c("Carwidth" = "blue", "Enginesize" = "green", "Stroke" = "black", 
                                "Compressionratio" = "red", "Peakrpm" = "purple")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_ea_m


```


Se puede observar que hay errores atípicos, los cuales podemos eliminar debido a que este análisis no es una serie de tiempo, en nuestro DF es el renglón 28, por lo que lo eliminamos

Graficamos para confirmar que no hay datos atípicos ahora 

```{r Sin Errores Atípicos}


df_sin_ea_m <- df_ea_m %>% 
  slice(-28)

grafico_sin_ea_m <- df_sin_ea_m %>% 
  ggplot() + 
  geom_hline(yintercept = -4, colour = "red") + 
  geom_hline(yintercept = 4, colour = "red")  +
  geom_point(aes(x = X1, y = Errores, color = "Carwidth")) + 
  geom_point(aes(x = X2, y = Errores, color = "Enginesize")) +
  geom_point(aes(x = X3, y = Errores, color = "Stroke")) +
  geom_point(aes(x = X4, y = Errores, color = "Compressionratio")) +
  geom_point(aes(x = log(X5), y = Errores, color = "Peakrpm")) + 
  ggtitle("x vs ui / raiz(mse)") +
  ylab("ui / raiz(mse)") +
  labs(title = "Sin datos atípicos") + 
  scale_color_manual(values = c("Carwidth" = "blue", "Enginesize" = "green", "Stroke" = "black", 
                                "Compressionratio" = "red", "Peakrpm" = "purple")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grafico_sin_ea_m

```

\pagebreak
\newpage

### Verificar la normalidad en los errores 

La QQ plot - esa se hace con los residuales sacamos los residuales de nuestro modelo

```{r QQ plot M}

ui_m <- as.data.frame(modelo.8$residuals)
colnames(ui_m) <- c("Ui")

ui_m %>% 
  ggplot(aes(sample = Ui)) + 
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot Residuales") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


```


Rechazamos el supuesto de normalidad de los errores debido a las dos colas que muestra el gráfico de QQ plot. No obstante, ya que el modelo de regresión lineal Multiple ajustado es robusto ante el supuesto de normalidad podemos continuar usando estas variable.

# Comparativo entre modelos y selección de un modelo



```{r R^2 MRLS, comment= "El modelo de regresión lineal simple tiene una R^2 ajustada de "}

suma_s <- summary(modelo_1)
suma_s$adj.r.squared

```


```{r R^2 MRLM, comment = "Y el MRLM tiene una R ajustada de "}

suma_m <- summary(modelo.8)
suma_m$r.squared


dif_R <- suma_m$adj.r.squared - suma_s$adj.r.squared

```

Ya que estamos utilizando las R^2 ajustadas, las cuales si nos dejan comparar modelos, y vemos que R^2 del MRLM es mayor que la del MRLS por:

```{r, comment = ""}

dif_R

```

Lo cual nos lleva a elegir el MRLM que es el mejor ajustado para nuestro precio.

# Conclusiones 


